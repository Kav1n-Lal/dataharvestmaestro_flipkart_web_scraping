{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\kavin\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\kavin\\anaconda3\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kavin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kavin\\AppData\\Local\\Temp\\ipykernel_14604\\1852715827.py:13: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "! pip install pandas\n",
    "! pip install webdriver-manager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function to scrape Laptop accessories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape data\n",
    "# User has to pass the 'component_name' in and the 'root_url' in strings in the function\n",
    "\n",
    "def scrape_func(component,root_url):\n",
    "    \n",
    "    \n",
    "    \n",
    "    a=[]\n",
    "    website=[]\n",
    "    brand_name=[]\n",
    "    rating=[]\n",
    "    rating_count=[]\n",
    "    reviews=[]\n",
    "    discount_price=[]\n",
    "    original_price=[]\n",
    "    discount_percent=[]\n",
    "    bank_offers=[]\n",
    "    final_spec=[]\n",
    "    delivery=[] \n",
    "    \n",
    "    \n",
    "    \n",
    "    def scrape(url):\n",
    "        # Collecting all the product links in one page and storing it in the list variable named 'links'\n",
    "        links=[]\n",
    "        for link in link_a_tags:\n",
    "            links.append(link.get_attribute(\"href\"))\n",
    "        #print(links)\n",
    "\n",
    "        #Taking one link from 'links' list and scraping the necessary data\n",
    "        for each_link in links:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                \n",
    "                #Loading the page\n",
    "                driver.get(each_link)\n",
    "                driver.implicitly_wait(3)\n",
    "\n",
    "                # Extracting brand_name \n",
    "                try:\n",
    "                    brand_name.append(driver.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]').text)\n",
    "                except:\n",
    "                    brand_name.append(np.nan)\n",
    "\n",
    "                # Extracting rating\n",
    "                try:\n",
    "                    rating.append(driver.find_element(By.XPATH,'//div[@class=\"_3LWZlK\"]').text)\n",
    "                except:\n",
    "                    rating.append(np.nan)\n",
    "                \n",
    "                # Extracting rating_count\n",
    "                try:\n",
    "                    rating_count.append(driver.find_element(By.XPATH,'(//div[@class=\"_3_L3jD\"]//span)[4]').text)\n",
    "                except:\n",
    "                    rating_count.append(np.nan)\n",
    "\n",
    "                # Extracting reviews\n",
    "                try:\n",
    "                    reviews.append(driver.find_element(By.XPATH,'(//div[@class=\"_3_L3jD\"]//span)[6]').text)\n",
    "                except:\n",
    "                    reviews.append(np.nan)\n",
    "\n",
    "                # Extracting discount price\n",
    "                try:\n",
    "                    discount_price.append(driver.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]').text)\n",
    "                except:\n",
    "                    discount_price.append(np.nan)\n",
    "\n",
    "                # Extracting original_price\n",
    "                try:\n",
    "                    o=driver.find_elements(By.XPATH,'//div[@class=\"_3I9_wc _2p6lqe\"]')\n",
    "                    op=[i.text for i in o]\n",
    "                    original_price.append(' '.join(op))\n",
    "                except:\n",
    "                    original_price.append(np.nan)\n",
    "\n",
    "                # Extracting dicount_percent\n",
    "                try:\n",
    "                    discount_percent.append(driver.find_element(By.XPATH,'//div[@class=\"_3Ay6Sb _31Dcoz\"]/span').text)\n",
    "                except:\n",
    "                    discount_percent.append(np.nan)\n",
    "\n",
    "                # Extracting bank_offers\n",
    "                try:\n",
    "                    d1=[]\n",
    "                    driver.find_element(By.XPATH,'//div[@class=\"IMZJg1\"]/span').click()\n",
    "                    r=1\n",
    "                    while r<len(driver.find_elements(By.XPATH,'//li[@class=\"_16eBzU col\"]')):\n",
    "                        \n",
    "                        try:\n",
    "                            k=f'(//li[@class=\"_16eBzU col\"])[{r}]/span'\n",
    "                            bk=driver.find_elements(By.XPATH,k)\n",
    "                            d=[i.text for i in bk]\n",
    "                            \n",
    "                            try:\n",
    "                                d.append(driver.find_element(By.XPATH,f'(//div[@ class=\"Bv11UC _1qNw3R\"])[{r}]').text)\n",
    "                                d1.append((' '.join(d)))\n",
    "                               \n",
    "                            except:\n",
    "                                d1.append((' '.join(d)))\n",
    "                                pass\n",
    "                        except:\n",
    "                            bank_offers.append('no offers')\n",
    "                        r+=1\n",
    "                    bank_offers.append(tuple(d1))\n",
    "                except:\n",
    "                    bank_offers.append(np.nan)\n",
    "\n",
    "                # Extracting delivery\n",
    "                try:\n",
    "                    d=[driver.find_element(By.XPATH,'(//div[@class=\"_3XINqE\"])[1]').text]\n",
    "                    delivery.append(d)\n",
    "                except:\n",
    "                    delivery.append(np.nan)\n",
    "\n",
    "                #website\n",
    "                website.append(each_link)\n",
    "                \n",
    "                \n",
    "\n",
    "                try:\n",
    "                    \n",
    "                    # Extracting specifications\n",
    "                    def scrape_spec():\n",
    "                        try:\n",
    "                            spec_container=driver.find_elements(By.XPATH,'//div[@class=\"flxcaE\"]')\n",
    "                            specification=[]\n",
    "                            sp=[]\n",
    "                            \n",
    "                            for j in range(1,len(spec_container)+1):\n",
    "                                spec=spec_container[j-1].text\n",
    "                               \n",
    "                                key=driver.find_elements(By.XPATH,f'(//table[@class=\"_14cfVK\"])[{j}]//td[@class=\"_1hKmbr col col-3-12\"]')\n",
    "                                value=driver.find_elements(By.XPATH,f'(//table[@class=\"_14cfVK\"])[{j}]//li[@class=\"_21lJbe\"]')\n",
    "                                #print(key)\n",
    "                                if len(key)==0:\n",
    "                                    sp.append(('0',value[0].text))\n",
    "                                else:\n",
    "                                    for i in range(len(key)):\n",
    "                                        sp.append((key[i].text,value[i].text))\n",
    "                              \n",
    "                                \n",
    "                                specification.append({spec:dict(sp)})\n",
    "                                sp.clear()\n",
    "                            final_spec.append(specification)\n",
    "                        except:\n",
    "                            final_spec.append(np.nan)\n",
    "                    try:\n",
    "                        button=driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]').text\n",
    "                        # print(len(button))\n",
    "                        \n",
    "                        #if len(button)>0:\n",
    "                        driver.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]').click()\n",
    "                        scrape_spec()\n",
    "                    except:\n",
    "                        key=driver.find_elements(By.XPATH,'(//table[@class=\"_14cfVK\"])//td[@class=\"_1hKmbr col col-3-12\"]')\n",
    "                        value=driver.find_elements(By.XPATH,'(//table[@class=\"_14cfVK\"])//li[@class=\"_21lJbe\"]')\n",
    "                        if len(key)!=0:\n",
    "                            specification=[]\n",
    "                            sp=[]\n",
    "                            \n",
    "                            if len(key)==0:\n",
    "                                sp.append(('0',value[0].text))\n",
    "                            else:\n",
    "                                for i in range(len(key)):\n",
    "                                    sp.append((key[i].text,value[i].text))\n",
    "                          \n",
    "                            \n",
    "                            specification.append({'specs':dict(sp)})\n",
    "                            sp.clear()\n",
    "                            final_spec.append(specification) \n",
    "                        else:\n",
    "                            final_spec.append(np.nan)\n",
    "                except:\n",
    "                    final_spec.append(np.nan)\n",
    "                        \n",
    "           \n",
    "                #final_spec.append(np.nan) \n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "            except:\n",
    "                a.append(each_link)\n",
    "                pass\n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "    # Using 'while loop' to move to the next page and scrape the data\n",
    "    page=1\n",
    "    while True:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        # Define the URL\n",
    "        \n",
    "        url = f\"{root_url}&page={page}\"\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            selected_page=list(driver.find_element(By.XPATH, '//a[@class=\"ge-49M _2Kfbh8\"]').get_attribute(\"href\"))\n",
    "            g=selected_page.index('=',-4)\n",
    "            print('scraping page no',''.join(selected_page[g+1:]),'........')\n",
    "            link_a_tags = driver.find_elements(By.XPATH, '//div[@class=\"_4ddWXP\"]//a[@class=\"_2rpwqI\"]')\n",
    "            \n",
    "            if (int(''.join(selected_page[g+1:]))==page) and ((len(link_a_tags))!=0):\n",
    "                scrape(url)\n",
    "    \n",
    "                #Checking whether we have scraped balanced data\n",
    "                print('data_length',[len(website), len(brand_name),len(rating),len(rating_count),\n",
    "                           len(reviews), len(discount_price),len(original_price), len(discount_percent),\n",
    "                           len(bank_offers), len(delivery),len(final_spec)])\n",
    "                \n",
    "                \n",
    "                print('page no',page,'scraped')\n",
    "                driver.quit()\n",
    "                page+=1\n",
    "            #else:\n",
    "                \n",
    "        except:\n",
    "                # Creating a dataframe with the collected data\n",
    "                df=pd.DataFrame({\n",
    "                'component':component,\n",
    "                'website':website,\n",
    "                'brand_name':brand_name,\n",
    "                'rating':rating,\n",
    "                'rating_count':rating_count,\n",
    "                'reviews':reviews,\n",
    "                'discount_price':discount_price,\n",
    "                'original_price':original_price,\n",
    "                'discount_percent':discount_percent,'bank_offers':bank_offers,\n",
    "                'delivery':delivery,'specification':final_spec})\n",
    "    \n",
    "                # Storing the dataframe as .csv file with the 'component' and 'page_no' as its name\n",
    "                df.to_csv(f'{component}_page{page}.csv',index=False)\n",
    "                print('-------------------------------------------------------------')\n",
    "                print('scraped until page_no',page-1)\n",
    "                driver.quit()\n",
    "                break\n",
    "        \n",
    "    print('errors',a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape UPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping 2\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping 3\n",
      "data_length [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
      "page no 3 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 3\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('ups','https://www.flipkart.com/laptop-accessories/ups/pr?sid=6bo,ai3,pi1&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape PENDRIVE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]\n",
      "page no 8 scraped\n",
      "scraping page no 9 ........\n",
      "data_length [360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360]\n",
      "page no 9 scraped\n",
      "scraping page no 10 ........\n",
      "data_length [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n",
      "page no 10 scraped\n",
      "scraping page no 11 ........\n",
      "data_length [440, 440, 440, 440, 440, 440, 440, 440, 440, 440, 440]\n",
      "page no 11 scraped\n",
      "scraping page no 12 ........\n",
      "data_length [480, 480, 480, 480, 480, 480, 480, 480, 480, 480, 480]\n",
      "page no 12 scraped\n",
      "scraping page no 13 ........\n",
      "data_length [520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520]\n",
      "page no 13 scraped\n",
      "scraping page no 14 ........\n",
      "data_length [560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560]\n",
      "page no 14 scraped\n",
      "scraping page no 15 ........\n",
      "data_length [600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "page no 15 scraped\n",
      "scraping page no 16 ........\n",
      "data_length [640, 640, 640, 640, 640, 640, 640, 640, 640, 640, 640]\n",
      "page no 16 scraped\n",
      "scraping page no 17 ........\n",
      "data_length [680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680]\n",
      "page no 17 scraped\n",
      "scraping page no 18 ........\n",
      "data_length [720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720]\n",
      "page no 18 scraped\n",
      "scraping page no 19 ........\n",
      "data_length [760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760]\n",
      "page no 19 scraped\n",
      "scraping page no 20 ........\n",
      "data_length [800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800]\n",
      "page no 20 scraped\n",
      "scraping page no 21 ........\n",
      "data_length [840, 840, 840, 840, 840, 840, 840, 840, 840, 840, 840]\n",
      "page no 21 scraped\n",
      "scraping page no 22 ........\n",
      "data_length [880, 880, 880, 880, 880, 880, 880, 880, 880, 880, 880]\n",
      "page no 22 scraped\n",
      "scraping page no 23 ........\n",
      "data_length [920, 920, 920, 920, 920, 920, 920, 920, 920, 920, 920]\n",
      "page no 23 scraped\n",
      "scraping page no 24 ........\n",
      "data_length [960, 960, 960, 960, 960, 960, 960, 960, 960, 960, 960]\n",
      "page no 24 scraped\n",
      "scraping page no 25 ........\n",
      "data_length [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "page no 25 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 25\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('pendrive','https://www.flipkart.com/computers/storage/pen-drives/pr?sid=6bo,jdy,uar&otracker=nmenu_sub_Electronics_0_Pendrives')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape MOUSE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]\n",
      "page no 8 scraped\n",
      "scraping page no 9 ........\n",
      "data_length [360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360]\n",
      "page no 9 scraped\n",
      "scraping page no 10 ........\n",
      "data_length [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n",
      "page no 10 scraped\n",
      "scraping page no 11 ........\n",
      "data_length [440, 440, 440, 440, 440, 440, 440, 440, 440, 440, 440]\n",
      "page no 11 scraped\n",
      "scraping page no 12 ........\n",
      "data_length [480, 480, 480, 480, 480, 480, 480, 480, 480, 480, 480]\n",
      "page no 12 scraped\n",
      "scraping page no 13 ........\n",
      "data_length [520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520]\n",
      "page no 13 scraped\n",
      "scraping page no 14 ........\n",
      "data_length [560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560]\n",
      "page no 14 scraped\n",
      "scraping page no 15 ........\n",
      "data_length [600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "page no 15 scraped\n",
      "scraping page no 16 ........\n",
      "data_length [640, 640, 640, 640, 640, 640, 640, 640, 640, 640, 640]\n",
      "page no 16 scraped\n",
      "scraping page no 17 ........\n",
      "data_length [680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680]\n",
      "page no 17 scraped\n",
      "scraping page no 18 ........\n",
      "data_length [720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720]\n",
      "page no 18 scraped\n",
      "scraping page no 19 ........\n",
      "data_length [760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760]\n",
      "page no 19 scraped\n",
      "scraping page no 20 ........\n",
      "data_length [800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800]\n",
      "page no 20 scraped\n",
      "scraping page no 21 ........\n",
      "data_length [840, 840, 840, 840, 840, 840, 840, 840, 840, 840, 840]\n",
      "page no 21 scraped\n",
      "scraping page no 22 ........\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 21\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('mouse','https://www.flipkart.com/laptop-accessories/mouse/pr?sid=6bo,ai3,2ay&otracker=nmenu_sub_Electronics_0_Mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape LAPTOP STAND Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping 2\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping 3\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping 4\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping 5\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping 6\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping 7\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping 8\n",
      "data_length [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]\n",
      "page no 8 scraped\n",
      "scraping 9\n",
      "data_length [360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360]\n",
      "page no 9 scraped\n",
      "scraping 10\n",
      "data_length [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n",
      "page no 10 scraped\n",
      "scraping 11\n",
      "data_length [440, 440, 440, 440, 440, 440, 440, 440, 440, 440, 440]\n",
      "page no 11 scraped\n",
      "scraping 12\n",
      "data_length [480, 480, 480, 480, 480, 480, 480, 480, 480, 480, 480]\n",
      "page no 12 scraped\n",
      "scraping 13\n",
      "data_length [520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520]\n",
      "page no 13 scraped\n",
      "scraping 14\n",
      "data_length [560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560]\n",
      "page no 14 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 14\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('laptop_stand','https://www.flipkart.com/search?q=laptop%20stand&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape KEYBOARD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]\n",
      "page no 8 scraped\n",
      "scraping page no 9 ........\n",
      "data_length [360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360]\n",
      "page no 9 scraped\n",
      "scraping page no 10 ........\n",
      "data_length [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n",
      "page no 10 scraped\n",
      "scraping page no 11 ........\n",
      "data_length [440, 440, 440, 440, 440, 440, 440, 440, 440, 440, 440]\n",
      "page no 11 scraped\n",
      "scraping page no 12 ........\n",
      "data_length [480, 480, 480, 480, 480, 480, 480, 480, 480, 480, 480]\n",
      "page no 12 scraped\n",
      "scraping page no 13 ........\n",
      "data_length [520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520]\n",
      "page no 13 scraped\n",
      "scraping page no 14 ........\n",
      "data_length [560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560]\n",
      "page no 14 scraped\n",
      "scraping page no 15 ........\n",
      "data_length [600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "page no 15 scraped\n",
      "scraping page no 16 ........\n",
      "data_length [640, 640, 640, 640, 640, 640, 640, 640, 640, 640, 640]\n",
      "page no 16 scraped\n",
      "scraping page no 17 ........\n",
      "data_length [680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680]\n",
      "page no 17 scraped\n",
      "scraping page no 18 ........\n",
      "data_length [720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720]\n",
      "page no 18 scraped\n",
      "scraping page no 19 ........\n",
      "data_length [760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760]\n",
      "page no 19 scraped\n",
      "scraping page no 20 ........\n",
      "data_length [800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800]\n",
      "page no 20 scraped\n",
      "scraping page no 21 ........\n",
      "data_length [840, 840, 840, 840, 840, 840, 840, 840, 840, 840, 840]\n",
      "page no 21 scraped\n",
      "scraping page no 22 ........\n",
      "data_length [880, 880, 880, 880, 880, 880, 880, 880, 880, 880, 880]\n",
      "page no 22 scraped\n",
      "scraping page no 23 ........\n",
      "data_length [920, 920, 920, 920, 920, 920, 920, 920, 920, 920, 920]\n",
      "page no 23 scraped\n",
      "scraping page no 24 ........\n",
      "data_length [960, 960, 960, 960, 960, 960, 960, 960, 960, 960, 960]\n",
      "page no 24 scraped\n",
      "scraping page no 25 ........\n",
      "data_length [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "page no 25 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 25\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('keyboard','https://www.flipkart.com/laptop-accessories/keyboards/pr?sid=6bo,ai3,3oe&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape POWERBANK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75]\n",
      "page no 2 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 2\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('laptop_powerbank','https://www.flipkart.com/laptop-accessories/power-banks/pr?sid=6bo,ai3,7m7&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape WEBCAM Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [93, 93, 93, 93, 93, 93, 93, 93, 93, 93, 93]\n",
      "page no 3 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 3\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('laptop_webcam','https://www.flipkart.com/laptop-accessories/webcams/pr?sid=6bo,ai3,r3e&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape PRINTER Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [281, 281, 281, 281, 281, 281, 281, 281, 281, 281, 281]\n",
      "page no 8 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 8\n",
      "errors ['https://www.flipkart.com/canon-pixma-megatank-g3012-multi-function-wifi-color-ink-tank-printer-color-page-cost-0-21-rs-black-0-09-borderless-printing/p/itma9118e839a8dc?pid=PRNF2QC6BX5M9NAJ&lid=LSTPRNF2QC6BX5M9NAJKRNVSY&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_242&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BXyAdE1KgcTXmM-QxP8Qq5j9ImSQQB-v_8waKhL6Cd69w%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/epson-ecotank-l6570-multi-function-wifi-color-ink-tank-printer-color-page-cost-36-paise-black-12-paise-ultra-high-yield-up-13500-pages-duplex-printing-4-3-lcd-touchscreen/p/itm1cbe9c592d607?pid=PRNGRWM93B456MAH&lid=LSTPRNGRWM93B456MAH4KGTET&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_243&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGRWM93B456MAH.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/retsol-pd-4000-multi-function-monochrome-laser-printer/p/itm487411cd388d9?pid=PRNGMNZFAV3HVNYM&lid=LSTPRNGMNZFAV3HVNYMEIPRU5&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_244&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGMNZFAV3HVNYM.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-color-laserjet-pro-m255dw-single-function-wifi-laser-printer/p/itm42aca341fb835?pid=PRNGMDMEZEHDQTTF&lid=LSTPRNGMDMEZEHDQTTFO5UW5A&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_245&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGMDMEZEHDQTTF.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/epson-m2110-multi-function-monochrome-ink-tank-printer-black-page-cost-15-paise/p/itm75911fd094da6?pid=PRNGDCYJHRZSYT8J&lid=LSTPRNGDCYJHRZSYT8JMQ1JQJ&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_246&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGDCYJHRZSYT8J.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-lbp6030w-single-function-wifi-monochrome-laser-printer/p/itmdz4pztngwny9h?pid=PRNDZ4ZUVEUBA6PJ&lid=LSTPRNDZ4ZUVEUBA6PJDL0B9R&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_247&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BWl0YZOk7XvcBMO3hu1DzfzJf8oxywzy2_qaLx65AeNAQ%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-pixma-ts3370s-multi-function-wifi-color-inkjet-printer-borderless-printing/p/itmb1d4822ec1a8c?pid=PRNFMSFZD8YTHHHH&lid=LSTPRNFMSFZD8YTHHHHPLABDD&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_248&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BU-1JWse7w-Weqc2qa1z9Hr0UlC5OJZF54d3KCJJrX7SA%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-ink-tank-319-all-in-one-colour-printer-multi-function-color/p/itm35fa754a8090e?pid=PRNGPZFJYHKYFVZG&lid=LSTPRNGPZFJYHKYFVZGTIC1EH&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_249&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGPZFJYHKYFVZG.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-3004dw-single-function-monochrome-laser-printer-white-grey-toner-cartridge/p/itm32c79a6da28e5?pid=PRNGZ782Y9M2FHGY&lid=LSTPRNGZ782Y9M2FHGYDENMYQ&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_250&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGZ782Y9M2FHGY.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/brother-dcp-b7535dw-multi-function-wifi-monochrome-laser-printer/p/itmfefaccbd2fd3c?pid=PRNFZZFZMPGXEEEH&lid=LSTPRNFZZFZMPGXEEEHJOG16M&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_251&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BXj5Fqb9DoqJRC-K9UJVydLzvucjGpsFysGGGdYspHbaw%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-pixma-megatank-g2012-multi-function-color-ink-tank-printer-color-page-cost-0-32-rs-black-0-09-rs/p/itmaa39d47ff54c3?pid=PRNFZZWTHNSR5UM7&lid=LSTPRNFZZWTHNSR5UM7MBGUWM&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_252&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BXFypJ6wJ2FeeO_iWcgFp2LXWjF1QCcnV6wjk3xHsehzw%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/brother-hl-b2000d-mono-laser-printer-single-function-monochrome/p/itm8b61992ff2660?pid=PRNGZ7YFAHDTVUFW&lid=LSTPRNGZ7YFAHDTVUFW0VVJAU&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_253&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGZ7YFAHDTVUFW.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-m405dw-single-function-monochrome-laser-printer/p/itmb444677f663b4?pid=PRNGHNBHYCZWF8FF&lid=LSTPRNGHNBHYCZWF8FF0U8HF2&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_254&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGHNBHYCZWF8FF.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/t-jet-12-7-mm-heavy-duty-handheld-thermal-inkjet-printer-batch-code-multi-function-monochrome/p/itm1887432fc558e?pid=PRNGUGJ4ACWY8KGX&lid=LSTPRNGUGJ4ACWY8KGXJTC5WS&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_255&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUGJ4ACWY8KGX.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-pixma-g2020-all-in-one-multi-function-color-inkjet-printer-ink-tank/p/itmdbc773f9cac81?pid=PRNGUF5RKYREQSFZ&lid=LSTPRNGUF5RKYREQSFZX2VRTF&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_256&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUF5RKYREQSFZ.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-imageclass-mf3010-multi-function-monochrome-laser-printer/p/itm39b12d68f700b?pid=PRND4PATTGPFW4HF&lid=LSTPRND4PATTGPFW4HFYAZMLA&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_257&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BUIB5MZgrtfBSTtG03tMMbvqrZRRqyRIiyRUcwkA_o1lw%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-pixma-megatank-g3770-multi-function-wifi-color-ink-tank-printer-black-135-ml-70-bottles/p/itmde2f80a86ca46?pid=PRNGNKWWJQXYATUH&lid=LSTPRNGNKWWJQXYATUHNID9JN&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_258&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BXJproWOX8JOdHrS43xouUFiQSVOiM3Fw__0mv1r68Hcg%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/xerox-wc-3025b-multi-function-wifi-monochrome-laser-printer/p/itmf23bd5d2db666?pid=PRNGGYZNGEYPBYH7&lid=LSTPRNGGYZNGEYPBYH7JLFCKU&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_259&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGGYZNGEYPBYH7.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/t-jet-pack-1-black-12-7-mm-touch-screen-thermal-inkjet-printer-printing-multi-function-monochrome/p/itmb4e8927fcc21a?pid=PRNGUGN29QZKTYEY&lid=LSTPRNGUGN29QZKTYEY5SOZC2&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_260&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUGN29QZKTYEY.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-smart-tank-all-one-529-multi-function-color-ink-printer/p/itm58e814aca5e48?pid=PRNGKZB6WHVACNP8&lid=LSTPRNGKZB6WHVACNP8NHWK0C&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_261&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BW5cs21WNQMvHQc9jJjw_ZwSbRhKAozMefY6JPLXQml0Q%3D%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-smart-tank-585-all-in-one-multi-function-wifi-color-ink-printer/p/itm0a59d663cdc12?pid=PRNGKZB67AFPX7GU&lid=LSTPRNGKZB67AFPX7GUPQSHJF&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_262&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BW2wZSOAOl3Ot8GpEfMzpWfkx0wcfauTgUFBApDDDlll4E8waCTrWcncKeK_ydPfoM%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-smart-tank-515-multi-function-wifi-color-ink-printer-voice-activated-printing-google-assistant-alexa-color-page-cost-20-paise-black-10-paise/p/itmda83d7a1d9744?pid=PRNFUSBVB3F5WFF6&lid=LSTPRNFUSBVB3F5WFF6U6YAYJ&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_263&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNFUSBVB3F5WFF6.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/epson-ecotank-m1140-single-function-monochrome-ink-tank-printer/p/itma2da0017ac5f8?pid=PRNGF449WNYSMBDF&lid=LSTPRNGF449WNYSMBDFNVR8YK&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_264&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGF449WNYSMBDF.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/t-jet-handheld-portable-mini-inkjet-printer-led-touch-screen-quick-drying-multi-function-monochrome/p/itm751ed85115fdc?pid=PRNGUGGPMB2FCJWB&lid=LSTPRNGUGGPMB2FCJWB9SITTR&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_265&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUGGPMB2FCJWB.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/t-jet-pack-2-handheld-portable-mini-inkjet-printer-led-touch-screen-multi-function-monochrome/p/itm8642313e4b898?pid=PRNGUGJHYYGVAGPT&lid=LSTPRNGUGJHYYGVAGPTDHVLJV&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_266&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUGJHYYGVAGPT.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-ink-tank-316-multi-function-color-printer-color-page-cost-20-paise-black-10-paise/p/itm832c91d32b77a?pid=PRNFUSBVNBFZQNDQ&lid=LSTPRNFUSBVNBFZQNDQYOGBOS&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_267&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BU-u5GgdiCzRQK4ymdUcYmh6ZxB3j_FAUbq_gHMPYAJDU4IsYyWu-Pj9cxFjFAoaLk%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-smart-tank-all-one-580-multi-function-wifi-color-ink-printer-print-scan-copy-1-additional-black-bottle-print-upto-12000-6000-pages-year-extended-warranty-pha-coverage/p/itm57b849f65df2c?pid=PRNGKZB6CXPAQWZS&lid=LSTPRNGKZB6CXPAQWZSUYPZX3&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_268&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BWOl_7K8jbOvd7acHRplbosm5TGm_5CKIM--aHPU-iT-4QEIsITtCzc4bHaOMTqL08%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/t-jet-1-inch-handheld-thermal-inkjet-printer-batch-code-plastic-bottle-pack-2-multi-function-monochrome/p/itmd8721cded8ac5?pid=PRNGUGKKXKTEK283&lid=LSTPRNGUGKKXKTEK283LVGYS6&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_269&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGUGKKXKTEK283.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-printer-scanner-copier-smart-colour-low-cost-per-page-multi-function-color-inkjet/p/itm44d63ec0d7ddd?pid=PRNGWFPHASJNDFQE&lid=LSTPRNGWFPHASJNDFQEMKUSX1&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_270&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGWFPHASJNDFQE.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-smart-tank-all-one-589-multi-function-wifi-color-ink-printer/p/itma1449e7e7cc32?pid=PRNGKZB6NY9YMRJN&lid=LSTPRNGKZB6NY9YMRJNZVXMPX&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_271&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BULfRF7AdRs8o-UCJ5oZa1tNsvebES0YndtmsQIlqWbdUKsf8s6I2Oz2HOgbXTo_9U%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/canon-pixma-g2020-all-in-one-multi-function-color-ink-tank-printer-color-page-cost-24-paise-black-13-paise/p/itmee7f3b6e66323?pid=PRNG3F3HVGMUP7WZ&lid=LSTPRNG3F3HVGMUP7WZ9TDKAU&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_272&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BWAPHxSUk9vUMeCcgeYG-Tt9saqzNAU-mzZMobLNCSaAOo15S0a61Wzzbg3qNOknKY%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-mfp-1188a-multi-function-monochrome-laser-printer-inkjet/p/itm3cff0fdc6f993?pid=PRNGVSEJHXE7ZGVD&lid=LSTPRNGVSEJHXE7ZGVDIWDODL&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_273&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGVSEJHXE7ZGVD.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-black-smart-tank-750-multi-function-color-inkjet-printer-print-scan-copy-wifi-ink/p/itmc7986ead9a6dd?pid=PRNGVR8AN9FNFBNH&lid=LSTPRNGVR8AN9FNFBNHPUGAML&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_274&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGVR8AN9FNFBNH.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-multi-function-color-inkjet-printer-page-cost-20-black-10-paise-wifi-monochrome-ink-tank-printer/p/itme4ebab326f0ce?pid=PRNGVP3E4TRRAFX7&lid=LSTPRNGVP3E4TRRAFX7DQPMFG&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_275&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGVP3E4TRRAFX7.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-m178nw-wireless-laser-color-multifunction-mobile-ethernet-wi-fi-printer-4zb96am-multi-function/p/itm4a73c9047da1d?pid=PRNGJGECGBZAGNJE&lid=LSTPRNGJGECGBZAGNJEDTBU98&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_276&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGJGECGBZAGNJE.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/brother-dcp-t226-multi-function-color-ink-tank-printer-borderless-printing-ideal-home-usage-print-scan-copy/p/itm90558cd38ed54?pid=PRNGFF8Y7THAMZZW&lid=LSTPRNGFF8Y7THAMZZWJQGOL4&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_277&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BUjpPopzLRPzIzdZCnbfNps8Nf6TK99SveXGgDFPK1bCkhz9c-MSIoFe8xMxkfU6dM%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/brother-dcp-l2541dw-ind-multi-function-wifi-monochrome-laser-printer/p/itm6ecc05f6683f4?pid=PRNEATFTWEFRCHUK&lid=LSTPRNEATFTWEFRCHUK7WZWM4&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_278&otracker=browse&iid=en_71UDNinLpFWEx666Fwk9sArGUjBP9Xerru_uetuM8BXY4EibPg_8MwelYOvYZuwbzZxGRKgd_9ac8b4CaBAf_4nfEY2D5bWfIUmTlXs_oiU%3D&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-printer-mfp-1188w-multi-function-wifi-monochrome-laser-inkjet/p/itm6b06840dbdfb6?pid=PRNGVSEVG32NXND4&lid=LSTPRNGVSEVG32NXND4ESHAQN&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_279&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGVSEVG32NXND4.SEARCH&ssid=l401yly3dc0000001706428251581', 'https://www.flipkart.com/hp-black-smart-tank-516-multi-function-color-inkjet-printer-print-scan-copy-ink/p/itmb54edf6ca479e?pid=PRNGVR7BR5ZRTRAK&lid=LSTPRNGVR7BR5ZRTRAKM2FJEY&marketplace=FLIPKART&store=6bo%2Ftia%2Fffn%2Ft64&srno=b_7_280&otracker=browse&iid=88344ae8-5aa4-4f6c-b7b3-d52807986a9c.PRNGVR7BR5ZRTRAK.SEARCH&ssid=l401yly3dc0000001706428251581']\n"
     ]
    }
   ],
   "source": [
    "scrape_func('printer','https://www.flipkart.com/computers/computer-peripherals/printers-inks/printers/pr?sid=6bo,tia,ffn,t64&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Calling the function to scrape WIRED_HEADPHONES Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [320, 320, 320, 320, 320, 320, 320, 320, 320, 320, 320]\n",
      "page no 8 scraped\n",
      "scraping page no 9 ........\n",
      "data_length [360, 360, 360, 360, 360, 360, 360, 360, 360, 360, 360]\n",
      "page no 9 scraped\n",
      "scraping page no 10 ........\n",
      "data_length [400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400]\n",
      "page no 10 scraped\n",
      "scraping page no 11 ........\n",
      "data_length [440, 440, 440, 440, 440, 440, 440, 440, 440, 440, 440]\n",
      "page no 11 scraped\n",
      "scraping page no 12 ........\n",
      "data_length [480, 480, 480, 480, 480, 480, 480, 480, 480, 480, 480]\n",
      "page no 12 scraped\n",
      "scraping page no 13 ........\n",
      "data_length [520, 520, 520, 520, 520, 520, 520, 520, 520, 520, 520]\n",
      "page no 13 scraped\n",
      "scraping page no 14 ........\n",
      "data_length [560, 560, 560, 560, 560, 560, 560, 560, 560, 560, 560]\n",
      "page no 14 scraped\n",
      "scraping page no 15 ........\n",
      "data_length [600, 600, 600, 600, 600, 600, 600, 600, 600, 600, 600]\n",
      "page no 15 scraped\n",
      "scraping page no 16 ........\n",
      "data_length [640, 640, 640, 640, 640, 640, 640, 640, 640, 640, 640]\n",
      "page no 16 scraped\n",
      "scraping page no 17 ........\n",
      "data_length [680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680]\n",
      "page no 17 scraped\n",
      "scraping page no 18 ........\n",
      "data_length [720, 720, 720, 720, 720, 720, 720, 720, 720, 720, 720]\n",
      "page no 18 scraped\n",
      "scraping page no 19 ........\n",
      "data_length [760, 760, 760, 760, 760, 760, 760, 760, 760, 760, 760]\n",
      "page no 19 scraped\n",
      "scraping page no 20 ........\n",
      "data_length [800, 800, 800, 800, 800, 800, 800, 800, 800, 800, 800]\n",
      "page no 20 scraped\n",
      "scraping page no 21 ........\n",
      "data_length [840, 840, 840, 840, 840, 840, 840, 840, 840, 840, 840]\n",
      "page no 21 scraped\n",
      "scraping page no 22 ........\n",
      "data_length [880, 880, 880, 880, 880, 880, 880, 880, 880, 880, 880]\n",
      "page no 22 scraped\n",
      "scraping page no 23 ........\n",
      "data_length [920, 920, 920, 920, 920, 920, 920, 920, 920, 920, 920]\n",
      "page no 23 scraped\n",
      "scraping page no 24 ........\n",
      "data_length [960, 960, 960, 960, 960, 960, 960, 960, 960, 960, 960]\n",
      "page no 24 scraped\n",
      "scraping page no 25 ........\n",
      "data_length [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "page no 25 scraped\n",
      "-------------------------------------------------------------\n",
      "scraped until page_no 25\n",
      "errors []\n"
     ]
    }
   ],
   "source": [
    "scrape_func('wired_headphones','https://www.flipkart.com/headset/headphones/wired-headphones/pr?sid=fcn,gc3,rph&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape WIRELESS_HEADPHONES Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page no 1 ........\n",
      "data_length [40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "page no 1 scraped\n",
      "scraping page no 2 ........\n",
      "data_length [80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80]\n",
      "page no 2 scraped\n",
      "scraping page no 3 ........\n",
      "data_length [120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120]\n",
      "page no 3 scraped\n",
      "scraping page no 4 ........\n",
      "data_length [160, 160, 160, 160, 160, 160, 160, 160, 160, 160, 160]\n",
      "page no 4 scraped\n",
      "scraping page no 5 ........\n",
      "data_length [200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "page no 5 scraped\n",
      "scraping page no 6 ........\n",
      "data_length [240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240]\n",
      "page no 6 scraped\n",
      "scraping page no 7 ........\n",
      "data_length [280, 280, 280, 280, 280, 280, 280, 280, 280, 280, 280]\n",
      "page no 7 scraped\n",
      "scraping page no 8 ........\n",
      "data_length [318, 318, 318, 318, 318, 318, 318, 318, 318, 318, 318]\n",
      "page no 8 scraped\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Could not reach host. Are you offline?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connection.py:203\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\socket.py:962\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    961\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    963\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connectionpool.py:491\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connectionpool.py:467\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connectionpool.py:1096\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connection.py:611\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    610\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    612\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connection.py:210\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x0000024C1E002FD0>: Failed to resolve 'googlechromelabs.github.io' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\urllib3\\util\\retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='googlechromelabs.github.io', port=443): Max retries exceeded with url: /chrome-for-testing/latest-patch-versions-per-build.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024C1E002FD0>: Failed to resolve 'googlechromelabs.github.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\http.py:32\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ssl_verify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\requests\\adapters.py:519\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='googlechromelabs.github.io', port=443): Max retries exceeded with url: /chrome-for-testing/latest-patch-versions-per-build.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024C1E002FD0>: Failed to resolve 'googlechromelabs.github.io' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwireless_headphones\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.flipkart.com/headset/headphones/wireless-headphones/pr?sid=fcn,gc3,ka8&otracker=categorytree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 195\u001b[0m, in \u001b[0;36mscrape_func\u001b[1;34m(component, root_url)\u001b[0m\n\u001b[0;32m    193\u001b[0m page\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mService(\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;66;03m# Define the URL\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&page=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:35\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[1;32m---> 35\u001b[0m     binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:105\u001b[0m, in \u001b[0;36mDriverCacheManager.find_driver\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m browser_version:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m driver_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cache_key_driver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_metadata_content()\n\u001b[0;32m    108\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_metadata_key(driver)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:152\u001b[0m, in \u001b[0;36mDriverCacheManager.get_cache_key_driver_version\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version\n\u001b[1;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_driver_version_to_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:48\u001b[0m, in \u001b[0;36mDriver.get_driver_version_to_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_latest_release_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py:59\u001b[0m, in \u001b[0;36mChromeDriver.get_latest_release_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m determined_browser_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(determined_browser_version) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m115\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     58\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://googlechromelabs.github.io/chrome-for-testing/latest-patch-versions-per-build.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     response_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     61\u001b[0m     determined_browser_version \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuilds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mget(determined_browser_version)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\flipkart_harvest\\Lib\\site-packages\\webdriver_manager\\core\\http.py:35\u001b[0m, in \u001b[0;36mWDMHttpClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     resp \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m     33\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_verify, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mConnectionError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not reach host. Are you offline?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_response(resp)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mConnectionError\u001b[0m: Could not reach host. Are you offline?"
     ]
    }
   ],
   "source": [
    "scrape_func('wireless_headphones','https://www.flipkart.com/headset/headphones/wireless-headphones/pr?sid=fcn,gc3,ka8&otracker=categorytree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the function to scrape NUMBER PAD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_func('number_pad','https://www.flipkart.com/laptop-accessories/number-pads/pr?sid=6bo,ai3,pvy&q=keyboard&otracker=categorytree')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flipkart_harvest",
   "language": "python",
   "name": "flipkart_harvest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
